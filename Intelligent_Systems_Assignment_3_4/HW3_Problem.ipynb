{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    global X\n",
    "    global Y\n",
    "    all_indexes = list(range(len(X)))\n",
    "    train_indexes = random.sample(all_indexes,4000)\n",
    "    test_indexes = list(set(all_indexes)-set(train_indexes))\n",
    "    train_X = [X[i] for i in train_indexes]\n",
    "    train_Y = [Y[i] for i in train_indexes]\n",
    "    test_X = [X[i] for i in test_indexes]\n",
    "    test_Y = [Y[i] for i in test_indexes]\n",
    "    return train_X,test_X,train_Y,test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(inputs,outputs,prop):\n",
    "    all_indexes = list(range(len(inputs)))\n",
    "    split = int(prop*len(inputs))\n",
    "    random_indexes = random.sample(all_indexes,split)\n",
    "    random_inputs = [inputs[i] for i in random_indexes]\n",
    "    random_outputs = [outputs[i] for i in random_indexes]\n",
    "    return random_inputs,random_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(point):\n",
    "    if len(point)==0:\n",
    "        return 0\n",
    "    for i in range(len(point)):\n",
    "        if point[i]==1:\n",
    "            return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(actuals,preds):\n",
    "    if len(actuals)!=len(preds):\n",
    "        print(\"Sizes don't match\")\n",
    "        return\n",
    "    if len(actuals[0])!=len(preds[0]):\n",
    "        print(\"Dimensions don't match\")\n",
    "        return\n",
    "    cm = np.zeros((len(actuals[0]),len(preds[0])))\n",
    "    for i in range(len(actuals)):\n",
    "        actual = get_label(actuals[i])\n",
    "        predicted = get_label(preds[i])\n",
    "        cm[actual][predicted] += 1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(preds,actuals):\n",
    "    if len(actuals)!=len(preds):\n",
    "        print(\"Sizes don't match\")\n",
    "        return\n",
    "    if len(actuals[0])!=len(preds[0]):\n",
    "        print(\"Dimensions don't match\")\n",
    "        return\n",
    "    total_error = 0.0\n",
    "    for i in range(len(actuals)):\n",
    "        total_error += np.sum(np.square(np.array(actuals[i])-np.array(preds[i])))\n",
    "    return total_error/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_error(preds,actuals,labels,query_label):\n",
    "    label_cnt = 0\n",
    "    if len(actuals)!=len(preds):\n",
    "        print(\"Sizes don't match\")\n",
    "        return\n",
    "    if len(actuals[0])!=len(preds[0]):\n",
    "        print(\"Dimensions don't match\")\n",
    "        return\n",
    "    total_error = 0.0\n",
    "    for i in range(len(actuals)):\n",
    "        actual_label = get_label(labels[i])\n",
    "        if(actual_label == query_label):\n",
    "            label_cnt += 1\n",
    "            total_error += np.sum(np.square(np.array(actuals[i])-np.array(preds[i])))\n",
    "    total_error = (total_error/label_cnt) #normalizing by label count\n",
    "    return total_error/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines read: 5000\n"
     ]
    }
   ],
   "source": [
    "fw = open(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//MNISTnumImages5000.txt\",\"r\")\n",
    "lines = fw.readlines()\n",
    "print(\"Number of lines read:\",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines:5000, number of features:785\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "for line in lines:\n",
    "    inputs = [float(i) for i in line.split(\"\\t\")]\n",
    "    inputs.insert(0,1) #bias weight\n",
    "    X.append(inputs)\n",
    "print(\"Number of lines:%d, number of features:%d\" %(len(X),len(X[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines read: 5000\n"
     ]
    }
   ],
   "source": [
    "fw = open(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//MNISTnumLabels5000.txt\",\"r\")\n",
    "lines = fw.readlines()\n",
    "print(\"Number of lines read:\",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines:5000, dimensions:10\n"
     ]
    }
   ],
   "source": [
    "Y=[]\n",
    "for line in lines:\n",
    "    nums = [0,1,2,3,4,5,6,7,8,9]\n",
    "    y = [1 if (num==int(line)) else 0 for num in nums]\n",
    "    Y.append(y)\n",
    "print(\"Number of lines:%d, dimensions:%d\" %(len(Y),len(Y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    #initializing initial weights(including bias)\n",
    "    def __init__(self,weights):\n",
    "        self.weights = weights\n",
    "        self.changes = np.zeros(len(weights)) #initial change initiated to all 0's\n",
    "        \n",
    "    #this function returns the sigmoid transformed output for the given input from a neuron\n",
    "    def get_output(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.net_input = self.compute()\n",
    "        self.output = self.sigmoid()\n",
    "        return self.output\n",
    "    \n",
    "    #this function computes to total net input over all input and weight values\n",
    "    def compute(self):\n",
    "        return np.dot(self.inputs,self.weights)\n",
    "    \n",
    "    #this function applies the sigmoid transformation to the total netinput\n",
    "    def sigmoid(self):\n",
    "        sig = 1 + math.exp(-self.net_input)\n",
    "        return 1/sig\n",
    "    \n",
    "    #this method computers the squared error for a neuron\n",
    "    def get_squared_error(self,actual_output):\n",
    "        return 0.5 * (actual_output - self.output)**2\n",
    "    \n",
    "    #this method calculated delta for the specific neuron(useful for output layer)\n",
    "    def calculate_delta(self,actual_output):\n",
    "        error = actual_output - self.output\n",
    "        differential = self.get_differential() #differential of sigmoid wrt to net input\n",
    "        return error * differential #delta = error * differential\n",
    "    \n",
    "    def get_differential(self):\n",
    "        return self.output*(1 - self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class represents a layer of neurons\n",
    "class Layer:\n",
    "    '''initialization function\n",
    "    neuron_cnt -> number of neurons in the layer\n",
    "    initial_weights -> initial set of weights to be assigned for the neuron layer(including bias)'''\n",
    "    def __init__(self,neuron_cnt,input_cnt):\n",
    "        self.neurons = [] #list of neurons belonging to the layer\n",
    "        for i in range(neuron_cnt):\n",
    "            initial_weights = self.generate_weights(neuron_cnt,input_cnt)\n",
    "            self.neurons.append(Neuron(initial_weights))\n",
    "            \n",
    "    #this method generates random gaussian weights\n",
    "    def generate_weights(self,n,cnt):\n",
    "        '''\n",
    "        cnt = number of weights to be generated\n",
    "        n = number of neurons\n",
    "        '''\n",
    "        std = np.sqrt(float(2/n))\n",
    "        return np.random.normal(0.0,std,cnt)\n",
    "    \n",
    "    def update_outputs(self,inputs):\n",
    "        results = []\n",
    "        for neuron in self.neurons:\n",
    "            results.append(neuron.get_output(inputs))\n",
    "        return results\n",
    "    def get_outputs(self):\n",
    "        results = []\n",
    "        for neuron in self.neurons:\n",
    "            results.append(neuron.output)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class represents the entire network\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1):\n",
    "        '''\n",
    "        num_h_layer -> number of hidden layers to be used in the neural network\n",
    "        num_outputs -> number of output neurons\n",
    "        lrate_h -> learning rate to learn ip2h_weights\n",
    "        lrate_o -> learning rate to learn h2o_weights\n",
    "        '''\n",
    "        self.lrate_h = lrate_h\n",
    "        self.lrate_o = lrate_o\n",
    "        self.alpha = alpha\n",
    "        self.num_h_layers = num_h_layers\n",
    "        self.num_outputs = num_outputs #number of output neurons\n",
    "        self.hidden_cnts = hidden_cnts #number of neurons in each hidden layer\n",
    "        self.input_dim = num_inputs #dimension of each input pattern\n",
    "        self.hidden_layers = [] #list of hidden layers being used\n",
    "        #print(\"Number of hidden layers:%d, Number of output neurons:%d, Input dimensions:%d\" %(self.num_h_layers,self.num_outputs,self.input_dim))\n",
    "        for i in range(self.num_h_layers): #initializing each hidden neuron layer\n",
    "            if i==0:\n",
    "                #initializing first hidden layer i.e input -> h1\n",
    "                self.hidden_layers.append(Layer(self.hidden_cnts[i],self.input_dim))\n",
    "            else:\n",
    "                #initializing other hidden layer i.e h1->h2->....\n",
    "                self.hidden_layers.append(Layer(self.hidden_cnts[i],self.hidden_cnts[i-1]))\n",
    "        #initializing output layers using the weights from last hidden layer\n",
    "        self.output_layer = Layer(self.num_outputs,self.hidden_cnts[self.num_h_layers-1])\n",
    "        \n",
    "    #this function performs the \"feedforward\" pass of the algorithm\n",
    "    def feed_forward(self,inputs):\n",
    "        hidden_outputs = {}#dict which accumulates outputs from all hidden layers\n",
    "        final_outputs = [] #list of outputs from output(result) neurons\n",
    "        for i in range(self.num_h_layers):\n",
    "            outputs = [] \n",
    "            if i==0:\n",
    "                #calculating outputs of first hidden layer using the Input pattern\n",
    "                outputs = self.hidden_layers[i].update_outputs(inputs)\n",
    "            else:\n",
    "                #calculating outputs of other hidden layers using inputs from previous hidden layer\n",
    "                outputs = self.hidden_layers[i].update_outputs(hidden_outputs[i])\n",
    "            hidden_outputs[i]= outputs\n",
    "        #calculating outputs of result neurons using inputs from the last hidden layer\n",
    "        final_outputs = self.output_layer.update_outputs(hidden_outputs[self.num_h_layers-1])\n",
    "        return final_outputs\n",
    "    \n",
    "    #this function calculates the deltai of output layer deltai = yi-yihat*fdash(si)\n",
    "    def calc_o_deltas(self,training_output):\n",
    "        output_deltas = []\n",
    "        for i in range(self.num_outputs):\n",
    "            output_deltas.append(self.output_layer.neurons[i].calculate_delta(training_output[i]))\n",
    "        return output_deltas\n",
    "    \n",
    "    #this function calculates the deltaj of hidden layer deltaj = sigma wij*deltai*fdash(sj)\n",
    "    def calc_h_deltas(self,output_deltas):\n",
    "        hidden_deltas = {} #dictionary of deltas for each hidden layer in the order of last to first\n",
    "        for i in range(self.num_h_layers-1,-1,-1): #looping over all hidden layers in descending order\n",
    "            deltas = []\n",
    "            if i==self.num_h_layers-1:\n",
    "                #calculating delta of last hidden neuron layer based on output layer\n",
    "                for j in range(self.hidden_cnts[i]):\n",
    "                    delta = 0.0\n",
    "                    for k in range(self.num_outputs):\n",
    "                        #calculating  sigmawij*deltai component\n",
    "                        delta += output_deltas[k] * self.output_layer.neurons[k].weights[j]\n",
    "                    # sigma wij*deltaj*fdash\n",
    "                    deltas.append(delta * self.hidden_layers[i].neurons[j].get_differential())\n",
    "            else:\n",
    "                #calculating delta of all other hidden neuron layers\n",
    "                for j in range(self.hidden_cnts[i]):\n",
    "                    delta = 0.0\n",
    "                    for k in range(self.hidden_cnts[i+1]):\n",
    "                        #calculating sigmawij*deltai component\n",
    "                        delta += hidden_deltas[i+1][k] * self.hidden_layers[i+1].neurons[k].weights[j]\n",
    "                    # sigma wij*deltaj*fdash\n",
    "                    deltas.append(delta * self.hidden_layers[i].neurons[j].get_differential())\n",
    "            hidden_deltas[i]=deltas\n",
    "        return hidden_deltas\n",
    "    \n",
    "    #this function updates the weights of output layer\n",
    "    def update_o_weights(self,output_deltas):\n",
    "        for i in range(self.num_outputs):\n",
    "            for j in range(len(self.output_layer.neurons[i].weights)):\n",
    "                #deltawij = etao*deltai*hj\n",
    "                weight_change = self.lrate_o*output_deltas[i]*self.output_layer.neurons[i].inputs[j]\n",
    "                momentum = self.alpha*self.output_layer.neurons[i].changes[j] #momentum step\n",
    "                self.output_layer.neurons[i].changes[j] = weight_change\n",
    "                self.output_layer.neurons[i].weights[j] += weight_change + momentum\n",
    "                \n",
    "    #this function updates the weights of hidden layers\n",
    "    def update_h_weights(self,hidden_deltas):\n",
    "        for i in range(self.num_h_layers):\n",
    "            for j in range(self.hidden_cnts[i]):\n",
    "                for k in range(len(self.hidden_layers[i].neurons[j].weights)):\n",
    "                    #deltawjk = etah*deltaj*xk\n",
    "                    weight_change = self.lrate_h*hidden_deltas[i][j]*self.hidden_layers[i].neurons[j].inputs[k]\n",
    "                    momentum = self.alpha*self.hidden_layers[i].neurons[j].changes[k] #momentum step\n",
    "                    self.hidden_layers[i].neurons[j].changes[k] = weight_change\n",
    "                    self.hidden_layers[i].neurons[j].weights[k] += weight_change + momentum\n",
    "                        \n",
    "    #this function performs the training process including \"feedforward\" and \"backpropagation\" step for each input\n",
    "    def back_propagation(self,training_input,training_outputs):\n",
    "        pred_output = self.feed_forward(training_input)\n",
    "        output_deltas = self.calc_o_deltas(training_outputs)\n",
    "        hidden_deltas = self.calc_h_deltas(output_deltas)\n",
    "        self.update_o_weights(output_deltas)\n",
    "        self.update_h_weights(hidden_deltas)\n",
    "    \n",
    "    #this function encodes the outputs as binary values\n",
    "    def encode_output(self,output,mode):\n",
    "        if len(output)==0:\n",
    "            return 0\n",
    "        decoded_output = []\n",
    "        if mode==\"none\":\n",
    "            return output\n",
    "        if mode==\"normal\": #encoding using normal thresholds of 0.75 and 0.25 for classes 1 and 0\n",
    "            for o in output:\n",
    "                if o>=0.75:\n",
    "                    decoded_output.append(1)\n",
    "                elif o<=0.25:\n",
    "                    decoded_output.append(0)\n",
    "                else:\n",
    "                    decoded_output.append(o)\n",
    "            return decoded_output\n",
    "        elif mode==\"max\":\n",
    "            max_val = max(output)\n",
    "            for o in output:\n",
    "                if o==max_val:\n",
    "                    decoded_output.append(1)\n",
    "                else:\n",
    "                    decoded_output.append(0)\n",
    "            return decoded_output\n",
    "    \n",
    "    #this function predicts the output for given set of inputs\n",
    "    def predict(self,test_inputs,mode):\n",
    "        if len(test_inputs)==0:\n",
    "            print(\"Input size is 0\")\n",
    "            return\n",
    "        predicted_outputs = []\n",
    "        for t in range(len(test_inputs)):\n",
    "            outputs = self.feed_forward(test_inputs[t])\n",
    "            encoded_outputs = self.encode_output(outputs,mode)\n",
    "            predicted_outputs.append(encoded_outputs)\n",
    "        return predicted_outputs\n",
    "    \n",
    "    #this function calculates total squared error across all training sets\n",
    "    def calculate_squared_error(self,training_outputs):\n",
    "        total_error = 0.0\n",
    "        for i in range(len(training_outputs)):\n",
    "            total_error += self.output_layer.neurons[i].get_squared_error(training_outputs[i])\n",
    "        return total_error\n",
    "    \n",
    "    #this function saves all weights\n",
    "    def save_weights(self,filename):\n",
    "        fw = open(filename,\"w\")\n",
    "        fw.write(\"*****HIDDEN TO OUTPUT******\")\n",
    "        for t in range(self.num_outputs):\n",
    "            weights = self.output_layer.neurons[t].weights\n",
    "            for w in weights:\n",
    "                fw.write(str(w))\n",
    "                fw.write(\" \")\n",
    "            fw.write(\"\\n\")\n",
    "        fw.write(\"*****INPUT TO HIDDEN******\")\n",
    "        for t in range(self.num_h_layers):\n",
    "            for i in range(self.hidden_cnts[t]):\n",
    "                weights = self.hidden_layers[t].neurons[i].weights\n",
    "                for w in weights:\n",
    "                    fw.write(str(w))\n",
    "                    fw.write(\" \")\n",
    "                fw.write(\"\\n\")\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hit_rate(predicted,actual):\n",
    "    if(len(predicted)!=len(actual)):\n",
    "        print(\"Lengths do not match\")\n",
    "    hit_cnt = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]==actual[i]:\n",
    "            hit_cnt += 1\n",
    "    return float(hit_cnt)/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayScale(weights):\n",
    "    grayscale = []\n",
    "    for i in range(1,29):\n",
    "        grayscale.append(weights[28*(i-1):28*i])\n",
    "    return grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 1000 4000 1000\n"
     ]
    }
   ],
   "source": [
    "[train_X,test_X,train_Y,test_Y] = train_test_split()\n",
    "print(len(train_X),len(test_X),len(train_Y),len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: 100\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Counts: 150\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Counts: 200\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Counts: 250\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Counts: 300\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#training hidden neuron counts\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "hidden_cnts = [100,150,200,250,300]\n",
    "train_rates = {}\n",
    "test_rates = {}\n",
    "num_epochs = 100\n",
    "for cnt in hidden_cnts:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"Counts:\",cnt)\n",
    "    nn = Network(0.05,0.05,0.1,1,[cnt],785,10)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],outputs[k])\n",
    "    train_preds = nn.predict(train_X,\"normal\")\n",
    "    train_rates[str(cnt)] = calc_hit_rate(train_preds,train_Y)\n",
    "    test_preds = nn.predict(test_X,\"max\")\n",
    "    test_rates[str(cnt)] = calc_hit_rate(test_preds,test_Y)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals,test_vals = [],[]\n",
    "for cnt in hidden_cnts:\n",
    "    train_vals.append(train_rates[str(cnt)])\n",
    "    test_vals.append(test_rates[str(cnt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100': 0.61, '150': 0.62875, '200': 0.657, '250': 0.676, '300': 0.68175}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100': 0.869, '150': 0.883, '200': 0.877, '250': 0.883, '300': 0.89}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hidden_cnts,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(hidden_cnts,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel(\"Hidden Counts\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Hidden Neuron Count vs Hit Rate(100 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//hidden_vs_hitrate2\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrate: 0.01\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.02\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.03\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.04\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.05\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.060000000000000005\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.06999999999999999\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.08\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.09\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "lrate: 0.09999999999999999\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#training learning rate\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "lrates = np.arange(0.01,0.110,0.01)\n",
    "train_rates = {}\n",
    "test_rates = {}\n",
    "num_epochs = 100\n",
    "for lrate in lrates:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"lrate:\",lrate)\n",
    "    nn = Network(lrate,lrate,0.1,1,[150],785,10)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],outputs[k])\n",
    "    train_preds = nn.predict(train_X,\"normal\")\n",
    "    train_rates[str(lrate)] = calc_hit_rate(train_preds,train_Y)\n",
    "    test_preds = nn.predict(test_X,\"max\")\n",
    "    test_rates[str(lrate)] = calc_hit_rate(test_preds,test_Y)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for lrate in lrates:\n",
    "    train_vals.append(train_rates[str(lrate)])\n",
    "    test_vals.append(test_rates[str(lrate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrates,train_vals,'-r',label=\"Training Hit Rate\")\n",
    "plt.plot(lrates,test_vals,'-b',label=\"Testing Hit Rate\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Learning Rate vs Hit Rate(100 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//learning_vs_hitrate\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.05\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "alpha: 0.1\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "alpha: 0.15\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "alpha: 0.2\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "alpha: 0.25\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Epoch: 60\n",
      "Epoch: 70\n",
      "Epoch: 80\n",
      "Epoch: 90\n",
      "Epoch: 100\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#training alpha value\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "alphas = [0.05,0.1,0.15,0.2,0.25]\n",
    "train_rates = {}\n",
    "test_rates = {}\n",
    "num_epochs = 100\n",
    "for alpha in alphas:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"alpha:\",alpha)\n",
    "    nn = Network(0.1,0.1,alpha,1,[150],785,10)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],outputs[k])\n",
    "    train_preds = nn.predict(train_X,\"normal\")\n",
    "    train_rates[str(alpha)] = calc_hit_rate(train_preds,train_Y)\n",
    "    test_preds = nn.predict(test_X,\"max\")\n",
    "    test_rates[str(alpha)] = calc_hit_rate(test_preds,test_Y)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for alpha in alphas:\n",
    "    train_vals.append(train_rates[str(alpha)])\n",
    "    test_vals.append(test_rates[str(alpha)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(alphas,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Alpha vs Hit Rate(100 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//alpha_vs_hitrate\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****starting****\n",
      "Epoch:10, hit_rate:0.520000\n",
      "Epoch:20, hit_rate:0.620000\n",
      "Epoch:30, hit_rate:0.700000\n",
      "Epoch:40, hit_rate:0.780000\n",
      "Epoch:50, hit_rate:0.730000\n",
      "Epoch:60, hit_rate:0.740000\n",
      "Epoch:70, hit_rate:0.720000\n",
      "Epoch:80, hit_rate:0.850000\n",
      "Epoch:90, hit_rate:0.840000\n",
      "Epoch:100, hit_rate:0.770000\n",
      "Epoch:110, hit_rate:0.770000\n",
      "Epoch:120, hit_rate:0.710000\n",
      "Epoch:130, hit_rate:0.780000\n",
      "Epoch:140, hit_rate:0.740000\n",
      "Epoch:150, hit_rate:0.810000\n",
      "Epoch:160, hit_rate:0.810000\n",
      "Epoch:170, hit_rate:0.850000\n",
      "Epoch:180, hit_rate:0.830000\n",
      "Epoch:190, hit_rate:0.910000\n",
      "Epoch:200, hit_rate:0.840000\n",
      "Epoch:210, hit_rate:0.870000\n",
      "Epoch:220, hit_rate:0.860000\n",
      "Epoch:230, hit_rate:0.920000\n",
      "Epoch:240, hit_rate:0.860000\n",
      "Epoch:250, hit_rate:0.890000\n",
      "Epoch:260, hit_rate:0.900000\n",
      "Epoch:270, hit_rate:0.920000\n",
      "Epoch:280, hit_rate:0.950000\n",
      "Epoch:290, hit_rate:0.930000\n",
      "Epoch:300, hit_rate:0.950000\n",
      "Epoch:310, hit_rate:0.940000\n",
      "Epoch:320, hit_rate:0.980000\n",
      "Epoch:330, hit_rate:0.900000\n",
      "Epoch:340, hit_rate:0.920000\n",
      "Epoch:350, hit_rate:0.950000\n",
      "Epoch:360, hit_rate:0.960000\n",
      "Epoch:370, hit_rate:0.930000\n",
      "Epoch:380, hit_rate:0.940000\n",
      "Epoch:390, hit_rate:0.950000\n",
      "Epoch:400, hit_rate:0.940000\n",
      "Epoch:410, hit_rate:0.940000\n",
      "Epoch:420, hit_rate:0.910000\n",
      "Epoch:430, hit_rate:0.910000\n",
      "Epoch:440, hit_rate:0.960000\n",
      "Epoch:450, hit_rate:0.930000\n",
      "Epoch:460, hit_rate:0.980000\n",
      "Epoch:470, hit_rate:0.940000\n",
      "Epoch:480, hit_rate:0.970000\n",
      "Epoch:490, hit_rate:0.990000\n",
      "Epoch:500, hit_rate:0.960000\n",
      "Epoch:510, hit_rate:0.970000\n",
      "Epoch:520, hit_rate:0.940000\n",
      "Epoch:530, hit_rate:0.950000\n",
      "Epoch:540, hit_rate:0.980000\n",
      "Epoch:550, hit_rate:0.950000\n",
      "Epoch:560, hit_rate:0.940000\n",
      "Epoch:570, hit_rate:0.970000\n",
      "Epoch:580, hit_rate:0.960000\n",
      "Epoch:590, hit_rate:0.960000\n",
      "Epoch:600, hit_rate:0.990000\n",
      "Epoch:610, hit_rate:0.940000\n",
      "Epoch:620, hit_rate:0.930000\n",
      "Epoch:630, hit_rate:0.970000\n",
      "Epoch:640, hit_rate:0.960000\n",
      "Epoch:650, hit_rate:0.980000\n",
      "Epoch:660, hit_rate:0.990000\n",
      "Epoch:670, hit_rate:1.000000\n",
      "Epoch:680, hit_rate:0.950000\n",
      "Epoch:690, hit_rate:0.970000\n",
      "Epoch:700, hit_rate:0.980000\n",
      "Epoch:710, hit_rate:0.980000\n",
      "Epoch:720, hit_rate:0.950000\n",
      "Epoch:730, hit_rate:0.990000\n",
      "Epoch:740, hit_rate:0.980000\n",
      "Epoch:750, hit_rate:0.990000\n",
      "Epoch:760, hit_rate:0.960000\n",
      "Epoch:770, hit_rate:0.980000\n",
      "Epoch:780, hit_rate:0.970000\n",
      "Epoch:790, hit_rate:0.990000\n",
      "Epoch:800, hit_rate:0.980000\n",
      "Epoch:810, hit_rate:0.970000\n",
      "Epoch:820, hit_rate:0.990000\n",
      "Epoch:830, hit_rate:0.950000\n",
      "Epoch:840, hit_rate:0.980000\n",
      "Epoch:850, hit_rate:0.990000\n",
      "Epoch:860, hit_rate:1.000000\n",
      "Epoch:870, hit_rate:0.990000\n",
      "Epoch:880, hit_rate:0.980000\n",
      "Epoch:890, hit_rate:1.000000\n",
      "Epoch:900, hit_rate:0.990000\n",
      "Epoch:910, hit_rate:0.970000\n",
      "Epoch:920, hit_rate:0.980000\n",
      "Epoch:930, hit_rate:0.990000\n",
      "Epoch:940, hit_rate:0.980000\n",
      "Epoch:950, hit_rate:0.970000\n",
      "Epoch:960, hit_rate:0.990000\n",
      "Epoch:970, hit_rate:0.960000\n",
      "Epoch:980, hit_rate:0.970000\n",
      "Epoch:990, hit_rate:0.980000\n",
      "Epoch:1000, hit_rate:0.960000\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#final training step using stochastic gradient descent\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "num_epochs = 1000\n",
    "hit_rates = []\n",
    "nn = Network(0.1,0.1,0.15,1,[150],785,10)#neural network with 400 random training points for each epoch\n",
    "print(\"****starting****\")\n",
    "for t in range(1,num_epochs+1):\n",
    "    [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "    training_hit_rate = 0.0\n",
    "    #epoch_start_time = timeit.default_timer()\n",
    "    for k in range(len(inputs)): #backpropagation for each point\n",
    "        nn.back_propagation(inputs[k],outputs[k])\n",
    "    if t%10==0: #calculating hit rate for every 10 epochs\n",
    "        predicted_outputs = nn.predict(inputs,\"normal\")\n",
    "        hit_rate = calc_hit_rate(predicted_outputs,outputs)\n",
    "        print(\"Epoch:%d, hit_rate:%f\" %(t,hit_rate))\n",
    "        hit_rates.append(hit_rate)\n",
    "nn.save_weights(\"final_weights_1000.txt\")\n",
    "train_preds = nn.predict(train_X,\"max\")\n",
    "train_cm = get_cm(train_Y,train_preds)\n",
    "test_preds = nn.predict(test_X,\"max\")\n",
    "test_cm = get_cm(test_Y,test_preds)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_pred_outputs_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-ba467e413fcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mfw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training_labels_max_1000.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_pred_outputs_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_pred_outputs_max' is not defined"
     ]
    }
   ],
   "source": [
    "#final hit-rates\n",
    "test_pred_outputs = nn.predict(test_X,\"max\")\n",
    "test_hit_rate = calc_hit_rate(test_pred_outputs,test_Y)\n",
    "\n",
    "train_pred_outputs = nn.predict(train_X,\"normal\")\n",
    "train_hit_rate = calc_hit_rate(train_pred_outputs,train_Y)\n",
    "\n",
    "#saving predicted labels for future use\n",
    "fw = open(\"Training_labels_1000.txt\",\"w\")\n",
    "for output in train_pred_outputs:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()\n",
    "\n",
    "\n",
    "fw = open(\"Training_labels_max_1000.txt\",\"w\")\n",
    "for output in train_pred_outputs_max:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_pred_outputs_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-cecf9aa6dbc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_pred_outputs_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_pred_outputs_max' is not defined"
     ]
    }
   ],
   "source": [
    "for output in train_pred_outputs_max:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving predicted labels for future use\n",
    "fw = open(\"Test_labels_1000.txt\",\"w\")\n",
    "for output in test_pred_outputs:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_outputs_max = nn.predict(train_X,\"max\")\n",
    "train_cm_max = get_cm(train_Y,train_pred_outputs_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors = 1 -np.array(hit_rates)\n",
    "plt.plot(range(1,1001,10),training_errors)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Time series of error rate(1000 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//error_timeseries(1000)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'0.08': 0.5155, '0.035': 0.46725, '0.06': 0.52425, '0.085': 0.526, '0.05': 0.51625, '0.065': 0.50625, \n",
    "#'0.005': 0.11875, '0.01': 0.23, '0.095': 0.54725, '0.03': 0.4385, '0.075': 0.53425, '0.045': 0.49175, \n",
    "#'0.04': 0.47, '0.02': 0.35725, '0.1': 0.53775, '0.055': 0.507, '0.09': 0.53975, '0.07': 0.5155, '0.015': 0.3165, '0.025': 0.4135}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'100': 0.4545, '200': 0.482, '150': 0.48975}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrate: 0.01\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.03\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.049999999999999996\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.06999999999999999\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.08999999999999998\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.10999999999999997\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.12999999999999998\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.15\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.16999999999999998\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "lrate: 0.18999999999999997\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#cross validating learning rates\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "lrates = np.arange(0.01,0.210,0.02)\n",
    "train_errors = {}\n",
    "test_errors = {}\n",
    "num_epochs = 50\n",
    "for lrate in lrates:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"lrate:\",lrate)\n",
    "    nn = Network(lrate,lrate,0.1,1,[150],785,785)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],inputs[k])\n",
    "    train_preds = nn.predict(train_X,\"none\")\n",
    "    train_errors[str(lrate)] = get_error(train_preds,train_X)\n",
    "    test_preds = nn.predict(test_X,\"none\")\n",
    "    test_errors[str(lrate)] = get_error(test_preds,test_X)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.01': 49737.723096442605,\n",
       " '0.03': 28626.399073702254,\n",
       " '0.049999999999999996': 22077.256217219157,\n",
       " '0.06999999999999999': 18593.20059331895,\n",
       " '0.08999999999999998': 16983.88059285223,\n",
       " '0.10999999999999997': 16462.507951581858,\n",
       " '0.12999999999999998': 15579.158615205157,\n",
       " '0.15': 15472.91558790051,\n",
       " '0.16999999999999998': 15852.442010356684,\n",
       " '0.18999999999999997': 16763.43881725486}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.01': 12579.989914866166,\n",
       " '0.03': 7377.350076512519,\n",
       " '0.049999999999999996': 5733.616687035704,\n",
       " '0.06999999999999999': 4912.177748141567,\n",
       " '0.08999999999999998': 4561.2731918726595,\n",
       " '0.10999999999999997': 4465.062955324469,\n",
       " '0.12999999999999998': 4261.066510454432,\n",
       " '0.15': 4228.206106523967,\n",
       " '0.16999999999999998': 4328.721311938149,\n",
       " '0.18999999999999997': 4568.781493429739}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for lrate in lrates:\n",
    "    train_vals.append(train_errors[str(lrate)])\n",
    "    test_vals.append(test_errors[str(lrate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrates = np.arange(0.01,0.210,0.02)\n",
    "train_vals = [99208.30,57307.14,43604.76,37749.79,34093.12,31666.06,31004.18,30736.009,32775.18,33264.38]\n",
    "test_vals = [25231.35,14910.60,11579.97,10199.35,9281.81,8720.66,8627.07,8548.89,9058.94,9310.46]\n",
    "train_vals = [vals/4000 for vals in train_vals]\n",
    "test_vals = [vals/1000 for vals in test_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrates,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(lrates,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Normalized Squared Error\")\n",
    "plt.title(\"Learning Rate vs Squared Error(50 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//lrate_vs_squarederror\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,301,50):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.05\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "alpha: 0.1\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "alpha: 0.15\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "alpha: 0.2\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "alpha: 0.25\n",
      "Epoch: 10\n",
      "Epoch: 20\n",
      "Epoch: 30\n",
      "Epoch: 40\n",
      "Epoch: 50\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#cross-validating alpha\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "alphas = [0.05,0.1,0.15,0.2,0.25]\n",
    "train_errors = {}\n",
    "test_errors = {}\n",
    "num_epochs = 50\n",
    "for alpha in alphas:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"alpha:\",alpha)\n",
    "    nn = Network(0.15,0.15,alpha,1,[150],785,785)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],inputs[k])\n",
    "    train_preds = nn.predict(train_X,\"none\")\n",
    "    train_errors[str(alpha)] = get_error(train_preds,train_X)\n",
    "    test_preds = nn.predict(test_X,\"none\")\n",
    "    test_errors[str(alpha)] = get_error(test_preds,test_X)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.05': 15881.359805515985,\n",
       " '0.1': 16029.223338981432,\n",
       " '0.15': 15386.697028266535,\n",
       " '0.2': 15894.496891939552,\n",
       " '0.25': 15379.620849707466}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.05': 4310.37578538298,\n",
       " '0.1': 4346.525779924942,\n",
       " '0.15': 4204.269722941959,\n",
       " '0.2': 4355.023360766859,\n",
       " '0.25': 4189.04801577654}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for alpha in alphas:\n",
    "    train_vals.append(train_errors[str(alpha)])\n",
    "    test_vals.append(test_errors[str(alpha)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.05,0.1,0.15,0.2,0.25]\n",
    "train_vals = [31101.07,31750.68,31264.08,30610.89,31794.04]\n",
    "test_vals = [8682.46,8806.08,8668.63,8638.38,8980.46]\n",
    "train_vals = [vals/4000 for vals in train_vals]\n",
    "test_vals = [vals/1000 for vals in test_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(alphas,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Normalized Squared Error\")\n",
    "plt.title(\"Alpha vs Squared Error(50 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//alpha_vs_squarederror\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****starting****\n",
      "Epoch:10, error:684.027974\n",
      "Epoch:20, error:455.492799\n",
      "Epoch:30, error:351.928509\n",
      "Epoch:40, error:306.446176\n",
      "Epoch:50, error:298.519030\n",
      "Epoch:60, error:258.145314\n",
      "Epoch:70, error:259.457654\n",
      "Epoch:80, error:235.729982\n",
      "Epoch:90, error:208.512434\n",
      "Epoch:100, error:194.218252\n",
      "Epoch:110, error:194.051207\n",
      "Epoch:120, error:201.930933\n",
      "Epoch:130, error:179.577333\n",
      "Epoch:140, error:176.585174\n",
      "Epoch:150, error:167.241141\n",
      "Epoch:160, error:191.636799\n",
      "Epoch:170, error:187.197814\n",
      "Epoch:180, error:180.490882\n",
      "Epoch:190, error:206.955010\n",
      "Epoch:200, error:153.161414\n",
      "Epoch:210, error:162.631806\n",
      "Epoch:220, error:168.066626\n",
      "Epoch:230, error:166.161925\n",
      "Epoch:240, error:144.669948\n",
      "Epoch:250, error:151.615147\n",
      "Epoch:260, error:152.183871\n",
      "Epoch:270, error:156.347988\n",
      "Epoch:280, error:146.935465\n",
      "Epoch:290, error:130.821999\n",
      "Epoch:300, error:140.714053\n",
      "Epoch:310, error:142.726391\n",
      "Epoch:320, error:144.692508\n",
      "Epoch:330, error:141.474866\n",
      "Epoch:340, error:129.827553\n",
      "Epoch:350, error:134.840946\n",
      "Epoch:360, error:125.345821\n",
      "Epoch:370, error:122.278434\n",
      "Epoch:380, error:129.437893\n",
      "Epoch:390, error:142.108763\n",
      "Epoch:400, error:126.603472\n",
      "Epoch:410, error:127.428284\n",
      "Epoch:420, error:130.030232\n",
      "Epoch:430, error:139.124401\n",
      "Epoch:440, error:127.750597\n",
      "Epoch:450, error:133.608869\n",
      "Epoch:460, error:118.267399\n",
      "Epoch:470, error:127.063994\n",
      "Epoch:480, error:131.049140\n",
      "Epoch:490, error:120.657459\n",
      "Epoch:500, error:121.825572\n",
      "Final Training Error:6628.865091 Testing Error:2526.849410\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#final training step using stochastic gradient descent\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[150],num_inputs=1,num_outputs=1\n",
    "num_epochs = 500\n",
    "errors = []\n",
    "nn = Network(0.15,0.15,0.20,1,[150],785,785)#neural network with 400 random training points for each epoch\n",
    "print(\"****starting****\")\n",
    "for t in range(1,num_epochs+1):\n",
    "    [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "    training_hit_rate = 0.0\n",
    "    #epoch_start_time = timeit.default_timer()\n",
    "    for k in range(len(inputs)): #backpropagation for each point\n",
    "        nn.back_propagation(inputs[k],inputs[k])\n",
    "    if t%10==0: #calculating hit rate for every 10 epochs\n",
    "        predicted_outputs = nn.predict(inputs,\"none\")\n",
    "        error = get_error(predicted_outputs,inputs)\n",
    "        print(\"Epoch:%d, error:%f\" %(t,error))\n",
    "        errors.append(error)\n",
    "nn.save_weights(\"auto_encoder_final_weights_500.txt\")\n",
    "train_preds = nn.predict(train_X,\"none\")\n",
    "train_error = get_error(train_preds,train_X)\n",
    "test_preds = nn.predict(test_X,\"none\")\n",
    "test_error = get_error(test_preds,test_X)\n",
    "print(\"Final Training Error:%f Testing Error:%f\" %(train_error,test_error))\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_weights = nn.get_hidden_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Error:6628.865091 Testing Error:2526.849410\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Training Error:%f Testing Error:%f\" %(train_error,test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-72b61b0ea552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnorm_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m784\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#normalizing errors by number of points\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnorm_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Normalized Squared Error per neuron\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time series of squared error(1000 Epochs)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3259\u001b[0m                       mplDeprecation)\n\u001b[0;32m   3260\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3261\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3262\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3263\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1717\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 243\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (50,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAKvCAYAAADul59JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG5VJREFUeJzt3V+oredd4PHvrzlGodYWzBmQJJqAp1MzRahzyHTohZV2hqQXyU2RBIpWQnMzUWYsQkSpEq9sGQpC/JMZS7VgY+yFHiSSC60oYkpO6UxoUgKH6DSHCD3WTG6KjZl55mLvKZudnezVk732afb5fODAetf7rLV/Nw9755v3XWvWWgEAAABwdXvTlR4AAAAAgCtPJAIAAABAJAIAAABAJAIAAAAgkQgAAACARCIAAAAA2iASzcynZuZrM/PlVzk/M/MbM3NhZp6cmR87+jEBAAAA2KZNriT6dHXba5y/vTqz++/e6rde/1gAAAAAHKdDI9Fa66+qf3qNJXdWv792PF69bWZ+4KgGBAAAAGD7Th3Be1xfPbfn+OLuc/+wf+HM3NvO1Ua9+c1v/rfveMc7juDHAwAAAFD1xS9+8R/XWqcv57VHEYnmgOfWQQvXWg9VD1WdPXt2nT9//gh+PAAAAABVM/O/Lve1R/HtZherG/cc31A9fwTvCwAAAMAxOYpIdK76qd1vOXt39eJa6xW3mgEAAADwnevQ281m5rPVe6vrZuZi9SvVd1WttX67erT6QHWh+kb1M9saFgAAAIDtODQSrbXuPuT8qv7TkU0EAAAAwLE7itvNAAAAAHiDE4kAAAAAEIkAAAAAEIkAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACANoxEM3PbzDwzMxdm5v4Dzv/gzHx+Zr40M0/OzAeOflQAAAAAtuXQSDQz11QPVrdXt1R3z8wt+5b9cvXIWutd1V3Vbx71oAAAAABszyZXEt1aXVhrPbvWeql6uLpz35pVfd/u47dWzx/diAAAAABs2yaR6PrquT3HF3ef2+tXqw/NzMXq0epnD3qjmbl3Zs7PzPlLly5dxrgAAAAAbMMmkWgOeG7tO767+vRa64bqA9VnZuYV773WemitdXatdfb06dPf/rQAAAAAbMUmkehideOe4xt65e1k91SPVK21/rb6nuq6oxgQAAAAgO3bJBI9UZ2ZmZtn5tp2Ppj63L41X63eVzUzP9JOJHI/GQAAAMAbxKGRaK31cnVf9Vj1lXa+xeypmXlgZu7YXfbR6iMz8z+rz1YfXmvtvyUNAAAAgO9QpzZZtNZ6tJ0PpN773Mf2PH66es/RjgYAAADAcdnkdjMAAAAATjiRCAAAAACRCAAAAACRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAANowEs3MbTPzzMxcmJn7X2XNT87M0zPz1Mz8wdGOCQAAAMA2nTpswcxcUz1Y/YfqYvXEzJxbaz29Z82Z6her96y1XpiZf7WtgQEAAAA4eptcSXRrdWGt9exa66Xq4erOfWs+Uj241nqhaq31taMdEwAAAIBt2iQSXV89t+f44u5ze729evvM/M3MPD4ztx30RjNz78ycn5nzly5duryJAQAAADhym0SiOeC5te/4VHWmem91d/XfZ+Ztr3jRWg+ttc6utc6ePn36250VAAAAgC3ZJBJdrG7cc3xD9fwBa/5krfUva62/q55pJxoBAAAA8AawSSR6ojozMzfPzLXVXdW5fWv+uPqJqpm5rp3bz549ykEBAAAA2J5DI9Fa6+Xqvuqx6ivVI2utp2bmgZm5Y3fZY9XXZ+bp6vPVL6y1vr6toQEAAAA4WrPW/o8XOh5nz55d58+fvyI/GwAAAOAkmpkvrrXOXs5rN7ndDAAAAIATTiQCAAAAQCQCAAAAQCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAAEokAAAAASCQCAAAAIJEIAAAAgEQiAAAAABKJAAAAAEgkAgAAACCRCAAAAIBEIgAAAAASiQAAAABIJAIAAAAgkQgAAACARCIAAAAA2jASzcxtM/PMzFyYmftfY90HZ2bNzNmjGxEAAACAbTs0Es3MNdWD1e3VLdXdM3PLAeveUv1c9YWjHhIAAACA7drkSqJbqwtrrWfXWi9VD1d3HrDu16qPV/98hPMBAAAAcAw2iUTXV8/tOb64+9y3zMy7qhvXWn/6Wm80M/fOzPmZOX/p0qVve1gAAAAAtmOTSDQHPLe+dXLmTdUnq48e9kZrrYfWWmfXWmdPnz69+ZQAAAAAbNUmkehideOe4xuq5/ccv6V6Z/WXM/P31burcz68GgAAAOCNY5NI9ER1ZmZunplrq7uqc///5FrrxbXWdWutm9ZaN1WPV3estc5vZWIAAAAAjtyhkWit9XJ1X/VY9ZXqkbXWUzPzwMzcse0BAQAAANi+U5ssWms9Wj2677mPvcra977+sQAAAAA4TpvcbgYAAADACScSAQAAACASAQAAACASAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQBtGopm5bWaemZkLM3P/Aed/fmaenpknZ+bPZ+aHjn5UAAAAALbl0Eg0M9dUD1a3V7dUd8/MLfuWfak6u9b60epz1cePelAAAAAAtmeTK4lurS6stZ5da71UPVzduXfBWuvza61v7B4+Xt1wtGMCAAAAsE2bRKLrq+f2HF/cfe7V3FP92UEnZubemTk/M+cvXbq0+ZQAAAAAbNUmkWgOeG4duHDmQ9XZ6hMHnV9rPbTWOrvWOnv69OnNpwQAAABgq05tsOZideOe4xuq5/cvmpn3V79U/fha65tHMx4AAAAAx2GTK4meqM7MzM0zc211V3Vu74KZeVf1O9Uda62vHf2YAAAAAGzToZForfVydV/1WPWV6pG11lMz88DM3LG77BPV91Z/NDP/Y2bOvcrbAQAAAPAdaJPbzVprPVo9uu+5j+15/P4jngsAAACAY7TJ7WYAAAAAnHAiEQAAAAAiEQAAAAAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAJBIBAAAAEAiEQAAAACJRAAAAAAkEgEAAACQSAQAAABAIhEAAAAAiUQAAAAAJBIBAAAAkEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAANCGkWhmbpuZZ2bmwszcf8D5756ZP9w9/4WZuemoBwUAAABgew6NRDNzTfVgdXt1S3X3zNyyb9k91QtrrR+uPln9+lEPCgAAAMD2bHIl0a3VhbXWs2utl6qHqzv3rbmz+r3dx5+r3jczc3RjAgAAALBNpzZYc3313J7ji9W/e7U1a62XZ+bF6vurf9y7aGbure7dPfzmzHz5coYGXpfr2rc3gWNh78GVY//BlWHvwZXxry/3hZtEooOuCFqXsaa11kPVQ1Uzc36tdXaDnw8cIXsPrgx7D64c+w+uDHsProyZOX+5r93kdrOL1Y17jm+onn+1NTNzqnpr9U+XOxQAAAAAx2uTSPREdWZmbp6Za6u7qnP71pyrfnr38Qerv1hrveJKIgAAAAC+Mx16u9nuZwzdVz1WXVN9aq311Mw8UJ1fa52rfrf6zMxcaOcKors2+NkPvY65gctn78GVYe/BlWP/wZVh78GVcdl7b1zwAwAAAMAmt5sBAAAAcMKJRAAAAABsPxLNzG0z88zMXJiZ+w84/90z84e7578wMzdteya4Gmyw935+Zp6emSdn5s9n5oeuxJxw0hy29/as++DMrJnx1cBwBDbZezPzk7u/+56amT847hnhpNrg784fnJnPz8yXdv/2/MCVmBNOkpn51Mx8bWa+/CrnZ2Z+Y3dfPjkzP7bJ+241Es3MNdWD1e3VLdXdM3PLvmX3VC+stX64+mT169ucCa4GG+69L1Vn11o/Wn2u+vjxTgknz4Z7r5l5S/Vz1ReOd0I4mTbZezNzpvrF6j1rrX9T/edjHxROoA1/9/1y9cha613tfMnRbx7vlHAifbq67TXO316d2f13b/Vbm7zptq8kurW6sNZ6dq31UvVwdee+NXdWv7f7+HPV+2ZmtjwXnHSH7r211ufXWt/YPXy8uuGYZ4STaJPfe1W/1k6Y/efjHA5OsE323keqB9daL1Sttb52zDPCSbXJ/lvV9+0+fmv1/DHOByfSWuuv2vl2+VdzZ/X7a8fj1dtm5gcOe99tR6Lrq+f2HF/cfe7ANWutl6sXq+/f8lxw0m2y9/a6p/qzrU4EV4dD997MvKu6ca31p8c5GJxwm/zee3v19pn5m5l5fGZe6/++ApvbZP/9avWhmblYPVr97PGMBle1b/e/Cas6tbVxdhx0RdC6jDXAt2fjfTUzH6rOVj++1Yng6vCae29m3tTOrdUfPq6B4Cqxye+9U+1ccv/edq6e/euZeeda639veTY46TbZf3dXn15r/deZ+ffVZ3b33//d/nhw1bqs1rLtK4kuVjfuOb6hV15a+K01M3OqncsPX+uSKeBwm+y9Zub91S9Vd6y1vnlMs8FJdtjee0v1zuovZ+bvq3dX53x4Nbxum/7N+SdrrX9Za/1d9Uw70Qh4fTbZf/dUj1Sttf62+p7qumOZDq5eG/034X7bjkRPVGdm5uaZubadDyk7t2/Nueqndx9/sPqLtZYrieD1OXTv7d7y8jvtBCKfywBH4zX33lrrxbXWdWutm9ZaN7XzeWB3rLXOX5lx4cTY5G/OP65+ompmrmvn9rNnj3VKOJk22X9frd5XNTM/0k4kunSsU8LV51z1U7vfcvbu6sW11j8c9qKt3m621np5Zu6rHquuqT611npqZh6ozq+1zlW/287lhhfauYLorm3OBFeDDffeJ6rvrf5o97Piv7rWuuOKDQ0nwIZ7DzhiG+69x6r/ODNPV/+n+oW11tev3NRwMmy4/z5a/beZ+S/t3O7yYRcGwOszM59t5xbq63Y/7+tXqu+qWmv9djuf//WB6kL1jepnNnpfexMAAACAbd9uBgAAAMAbgEgEAAAAgEgEAAAAgEgEAAAAQCIRAAAAAIlEAAAAACQSAQAAAFD9P6M5tksNhz2iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17307638208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_errors = [(error/100)/784 for error in errors] #normalizing errors by number of points\n",
    "plt.plot(range(1,1001,10),norm_errors)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Normalized Squared Error per neuron\")\n",
    "plt.title(\"Time series of squared error(1000 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//squarederror_timeseries(1000)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and testing errors\n",
    "train_error = (train_error)/4000 #normalizing the squared error\n",
    "test_error = (test_error)/1000\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "y_pos =[0.5,1]\n",
    "bars = [train_error,test_error]\n",
    "plt.bar(y_pos, bars, align='center', alpha=0.4,width=0.1)\n",
    "plt.xticks(y_pos,[\"Training Error\",\"Testing Error\"])\n",
    "plt.ylabel('Normalized Squared Error')\n",
    "plt.title('Training & Testing Squared Errors')\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//squared_error_bars(1000).png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_errors = []\n",
    "labeled_test_errors = []\n",
    "for i in range(10):\n",
    "    tr_error = get_labeled_error(train_preds,train_X,train_Y,i)\n",
    "    labeled_train_errors.append(tr_error)\n",
    "    te_error = get_labeled_error(test_preds,test_X,test_Y,i)\n",
    "    labeled_test_errors.append(te_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(1,11)\n",
    "labels = np.arange(10)\n",
    "plt.bar(y_pos,labeled_train_errors,align='center',alpha=0.4,width=0.3,color='r')\n",
    "plt.bar(y_pos+0.3,labeled_test_errors,align='center',alpha=0.4,width=0.3,color='y')\n",
    "plt.legend(('Training','Testing'))\n",
    "plt.xticks(y_pos,labels)\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Normalized Squared Error')\n",
    "plt.title('Digit-wise Squared Error')\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//digitwise_bars(1000).png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for i in range(150):\n",
    "    all_weights.append(nn.hidden_layers[0].neurons[i].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = all_weights[0]\n",
    "sample_weights = sample_weights[1:785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "gray_scale_weights = []\n",
    "for i in range(10):\n",
    "    gray_scale_weights.append(convert_to_grayScale(all_weights[i]))\n",
    "print(len(gray_scale_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,12)\n",
    "fig = plt.figure()\n",
    "grids = gridspec.GridSpec(10, 10, wspace=0.0)\n",
    "ax = [plt.subplot(grids[i]) for i in range(100)]\n",
    "grids.update(hspace=0)\n",
    "for i in range(100):\n",
    "    ax[i].imshow(convert_to_grayScale(all_weights[i]),cmap = plt.get_cmap('gray'))\n",
    "    ax[i].axis('off')\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//feature_grid(1000).png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
