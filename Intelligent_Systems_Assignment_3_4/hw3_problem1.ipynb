{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    global X\n",
    "    global Y\n",
    "    all_indexes = list(range(len(X)))\n",
    "    train_indexes = random.sample(all_indexes,4000)\n",
    "    test_indexes = list(set(all_indexes)-set(train_indexes))\n",
    "    train_X = [X[i] for i in train_indexes]\n",
    "    train_Y = [Y[i] for i in train_indexes]\n",
    "    test_X = [X[i] for i in test_indexes]\n",
    "    test_Y = [Y[i] for i in test_indexes]\n",
    "    return train_X,test_X,train_Y,test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(inputs,outputs,prop):\n",
    "    all_indexes = list(range(len(inputs)))\n",
    "    split = int(prop*len(inputs))\n",
    "    random_indexes = random.sample(all_indexes,split)\n",
    "    random_inputs = [inputs[i] for i in random_indexes]\n",
    "    random_outputs = [outputs[i] for i in random_indexes]\n",
    "    return random_inputs,random_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(point):\n",
    "    if len(point)==0:\n",
    "        return 0\n",
    "    for i in range(len(point)):\n",
    "        if point[i]==1:\n",
    "            return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(actuals,preds):\n",
    "    if len(actuals)!=len(preds):\n",
    "        print(\"Sizes don't match\")\n",
    "        return\n",
    "    if len(actuals[0])!=len(preds[0]):\n",
    "        print(\"Dimensions don't match\")\n",
    "        return\n",
    "    cm = np.zeros((len(actuals[0]),len(preds[0])))\n",
    "    for i in range(len(actuals)):\n",
    "        actual = get_label(actuals[i])\n",
    "        predicted = get_label(preds[i])\n",
    "        cm[actual][predicted] += 1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(preds,actuals):\n",
    "    if len(actuals)!=len(preds):\n",
    "        print(\"Sizes don't match\")\n",
    "        return\n",
    "    if len(actuals[0])!=len(preds[0]):\n",
    "        print(\"Dimensions don't match\")\n",
    "        return\n",
    "    total_error = 0.0\n",
    "    for i in range(len(actuals)):\n",
    "        total_error += np.sum(np.square(np.array(actuals[i])-np.array(preds[i])))\n",
    "    return total_error/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_error(preds,actuals,labels,query_label):\n",
    "    label_cnt = 0\n",
    "    if len(actuals)!=len(preds):\n",
    "        print(\"Sizes don't match\")\n",
    "        return\n",
    "    if len(actuals[0])!=len(preds[0]):\n",
    "        print(\"Dimensions don't match\")\n",
    "        return\n",
    "    total_error = 0.0\n",
    "    for i in range(len(actuals)):\n",
    "        actual_label = get_label(labels[i])\n",
    "        if(actual_label == query_label):\n",
    "            label_cnt += 1\n",
    "            total_error += np.sum(np.square(np.array(actuals[i])-np.array(preds[i])))\n",
    "    total_error = (total_error/label_cnt) #normalizing by label count\n",
    "    return total_error/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//MNISTnumImages5000.txt\",\"r\")\n",
    "lines = fw.readlines()\n",
    "print(\"Number of lines read:\",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for line in lines:\n",
    "    inputs = [float(i) for i in line.split(\"\\t\")]\n",
    "    inputs.insert(0,1) #bias weight\n",
    "    X.append(inputs)\n",
    "print(\"Number of lines:%d, number of features:%d\" %(len(X),len(X[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//MNISTnumLabels5000.txt\",\"r\")\n",
    "lines = fw.readlines()\n",
    "print(\"Number of lines read:\",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=[]\n",
    "for line in lines:\n",
    "    nums = [0,1,2,3,4,5,6,7,8,9]\n",
    "    y = [1 if (num==int(line)) else 0 for num in nums]\n",
    "    Y.append(y)\n",
    "print(\"Number of lines:%d, dimensions:%d\" %(len(Y),len(Y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    #initializing initial weights(including bias)\n",
    "    def __init__(self,weights):\n",
    "        self.weights = weights\n",
    "        self.changes = np.zeros(len(weights)) #initial change initiated to all 0's\n",
    "        \n",
    "    #this function returns the sigmoid transformed output for the given input from a neuron\n",
    "    def get_output(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.net_input = self.compute()\n",
    "        self.output = self.sigmoid()\n",
    "        return self.output\n",
    "    \n",
    "    #this function computes to total net input over all input and weight values\n",
    "    def compute(self):\n",
    "        return np.dot(self.inputs,self.weights)\n",
    "    \n",
    "    #this function applies the sigmoid transformation to the total netinput\n",
    "    def sigmoid(self):\n",
    "        sig = 1 + math.exp(-self.net_input)\n",
    "        return 1/sig\n",
    "    \n",
    "    #this method computers the squared error for a neuron\n",
    "    def get_squared_error(self,actual_output):\n",
    "        return 0.5 * (actual_output - self.output)**2\n",
    "    \n",
    "    #this method calculated delta for the specific neuron(useful for output layer)\n",
    "    def calculate_delta(self,actual_output):\n",
    "        error = actual_output - self.output\n",
    "        differential = self.get_differential() #differential of sigmoid wrt to net input\n",
    "        return error * differential #delta = error * differential\n",
    "    \n",
    "    def get_differential(self):\n",
    "        return self.output*(1 - self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class represents a layer of neurons\n",
    "class Layer:\n",
    "    '''initialization function\n",
    "    neuron_cnt -> number of neurons in the layer\n",
    "    initial_weights -> initial set of weights to be assigned for the neuron layer(including bias)'''\n",
    "    def __init__(self,neuron_cnt,input_cnt):\n",
    "        self.neurons = [] #list of neurons belonging to the layer\n",
    "        for i in range(neuron_cnt):\n",
    "            initial_weights = self.generate_weights(neuron_cnt,input_cnt)\n",
    "            self.neurons.append(Neuron(initial_weights))\n",
    "            \n",
    "    #this method generates random gaussian weights\n",
    "    def generate_weights(self,n,cnt):\n",
    "        '''\n",
    "        cnt = number of weights to be generated\n",
    "        n = number of neurons\n",
    "        '''\n",
    "        std = np.sqrt(float(2/n))\n",
    "        return np.random.normal(0.0,std,cnt)\n",
    "    \n",
    "    def update_outputs(self,inputs):\n",
    "        results = []\n",
    "        for neuron in self.neurons:\n",
    "            results.append(neuron.get_output(inputs))\n",
    "        return results\n",
    "    def get_outputs(self):\n",
    "        results = []\n",
    "        for neuron in self.neurons:\n",
    "            results.append(neuron.output)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class represents the entire network\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1):\n",
    "        '''\n",
    "        num_h_layer -> number of hidden layers to be used in the neural network\n",
    "        num_outputs -> number of output neurons\n",
    "        lrate_h -> learning rate to learn ip2h_weights\n",
    "        lrate_o -> learning rate to learn h2o_weights\n",
    "        '''\n",
    "        self.lrate_h = lrate_h\n",
    "        self.lrate_o = lrate_o\n",
    "        self.alpha = alpha\n",
    "        self.num_h_layers = num_h_layers\n",
    "        self.num_outputs = num_outputs #number of output neurons\n",
    "        self.hidden_cnts = hidden_cnts #number of neurons in each hidden layer\n",
    "        self.input_dim = num_inputs #dimension of each input pattern\n",
    "        self.hidden_layers = [] #list of hidden layers being used\n",
    "        #print(\"Number of hidden layers:%d, Number of output neurons:%d, Input dimensions:%d\" %(self.num_h_layers,self.num_outputs,self.input_dim))\n",
    "        for i in range(self.num_h_layers): #initializing each hidden neuron layer\n",
    "            if i==0:\n",
    "                #initializing first hidden layer i.e input -> h1\n",
    "                self.hidden_layers.append(Layer(self.hidden_cnts[i],self.input_dim))\n",
    "            else:\n",
    "                #initializing other hidden layer i.e h1->h2->....\n",
    "                self.hidden_layers.append(Layer(self.hidden_cnts[i],self.hidden_cnts[i-1]))\n",
    "        #initializing output layers using the weights from last hidden layer\n",
    "        self.output_layer = Layer(self.num_outputs,self.hidden_cnts[self.num_h_layers-1])\n",
    "        \n",
    "    #this function performs the \"feedforward\" pass of the algorithm\n",
    "    def feed_forward(self,inputs):\n",
    "        hidden_outputs = {}#dict which accumulates outputs from all hidden layers\n",
    "        final_outputs = [] #list of outputs from output(result) neurons\n",
    "        for i in range(self.num_h_layers):\n",
    "            outputs = [] \n",
    "            if i==0:\n",
    "                #calculating outputs of first hidden layer using the Input pattern\n",
    "                outputs = self.hidden_layers[i].update_outputs(inputs)\n",
    "            else:\n",
    "                #calculating outputs of other hidden layers using inputs from previous hidden layer\n",
    "                outputs = self.hidden_layers[i].update_outputs(hidden_outputs[i])\n",
    "            hidden_outputs[i]= outputs\n",
    "        #calculating outputs of result neurons using inputs from the last hidden layer\n",
    "        final_outputs = self.output_layer.update_outputs(hidden_outputs[self.num_h_layers-1])\n",
    "        return final_outputs\n",
    "    \n",
    "    #this function calculates the deltai of output layer deltai = yi-yihat*fdash(si)\n",
    "    def calc_o_deltas(self,training_output):\n",
    "        output_deltas = []\n",
    "        for i in range(self.num_outputs):\n",
    "            output_deltas.append(self.output_layer.neurons[i].calculate_delta(training_output[i]))\n",
    "        return output_deltas\n",
    "    \n",
    "    #this function calculates the deltaj of hidden layer deltaj = sigma wij*deltai*fdash(sj)\n",
    "    def calc_h_deltas(self,output_deltas):\n",
    "        hidden_deltas = {} #dictionary of deltas for each hidden layer in the order of last to first\n",
    "        for i in range(self.num_h_layers-1,-1,-1): #looping over all hidden layers in descending order\n",
    "            deltas = []\n",
    "            if i==self.num_h_layers-1:\n",
    "                #calculating delta of last hidden neuron layer based on output layer\n",
    "                for j in range(self.hidden_cnts[i]):\n",
    "                    delta = 0.0\n",
    "                    for k in range(self.num_outputs):\n",
    "                        #calculating  sigmawij*deltai component\n",
    "                        delta += output_deltas[k] * self.output_layer.neurons[k].weights[j]\n",
    "                    # sigma wij*deltaj*fdash\n",
    "                    deltas.append(delta * self.hidden_layers[i].neurons[j].get_differential())\n",
    "            else:\n",
    "                #calculating delta of all other hidden neuron layers\n",
    "                for j in range(self.hidden_cnts[i]):\n",
    "                    delta = 0.0\n",
    "                    for k in range(self.hidden_cnts[i+1]):\n",
    "                        #calculating sigmawij*deltai component\n",
    "                        delta += hidden_deltas[i+1][k] * self.hidden_layers[i+1].neurons[k].weights[j]\n",
    "                    # sigma wij*deltaj*fdash\n",
    "                    deltas.append(delta * self.hidden_layers[i].neurons[j].get_differential())\n",
    "            hidden_deltas[i]=deltas\n",
    "        return hidden_deltas\n",
    "    \n",
    "    #this function updates the weights of output layer\n",
    "    def update_o_weights(self,output_deltas):\n",
    "        for i in range(self.num_outputs):\n",
    "            for j in range(len(self.output_layer.neurons[i].weights)):\n",
    "                #deltawij = etao*deltai*hj\n",
    "                weight_change = self.lrate_o*output_deltas[i]*self.output_layer.neurons[i].inputs[j]\n",
    "                momentum = self.alpha*self.output_layer.neurons[i].changes[j] #momentum step\n",
    "                self.output_layer.neurons[i].changes[j] = weight_change\n",
    "                self.output_layer.neurons[i].weights[j] += weight_change + momentum\n",
    "                \n",
    "    #this function updates the weights of hidden layers\n",
    "    def update_h_weights(self,hidden_deltas):\n",
    "        for i in range(self.num_h_layers):\n",
    "            for j in range(self.hidden_cnts[i]):\n",
    "                for k in range(len(self.hidden_layers[i].neurons[j].weights)):\n",
    "                    #deltawjk = etah*deltaj*xk\n",
    "                    weight_change = self.lrate_h*hidden_deltas[i][j]*self.hidden_layers[i].neurons[j].inputs[k]\n",
    "                    momentum = self.alpha*self.hidden_layers[i].neurons[j].changes[k] #momentum step\n",
    "                    self.hidden_layers[i].neurons[j].changes[k] = weight_change\n",
    "                    self.hidden_layers[i].neurons[j].weights[k] += weight_change + momentum\n",
    "                        \n",
    "    #this function performs the training process including \"feedforward\" and \"backpropagation\" step for each input\n",
    "    def back_propagation(self,training_input,training_outputs):\n",
    "        pred_output = self.feed_forward(training_input)\n",
    "        output_deltas = self.calc_o_deltas(training_outputs)\n",
    "        hidden_deltas = self.calc_h_deltas(output_deltas)\n",
    "        self.update_o_weights(output_deltas)\n",
    "        self.update_h_weights(hidden_deltas)\n",
    "    \n",
    "    #this function encodes the outputs as binary values\n",
    "    def encode_output(self,output,mode):\n",
    "        if len(output)==0:\n",
    "            return 0\n",
    "        decoded_output = []\n",
    "        if mode==\"none\":\n",
    "            return output\n",
    "        if mode==\"normal\": #encoding using normal thresholds of 0.75 and 0.25 for classes 1 and 0\n",
    "            for o in output:\n",
    "                if o>=0.75:\n",
    "                    decoded_output.append(1)\n",
    "                elif o<=0.25:\n",
    "                    decoded_output.append(0)\n",
    "                else:\n",
    "                    decoded_output.append(o)\n",
    "            return decoded_output\n",
    "        elif mode==\"max\":\n",
    "            max_val = max(output)\n",
    "            for o in output:\n",
    "                if o==max_val:\n",
    "                    decoded_output.append(1)\n",
    "                else:\n",
    "                    decoded_output.append(0)\n",
    "            return decoded_output\n",
    "    \n",
    "    #this function predicts the output for given set of inputs\n",
    "    def predict(self,test_inputs,mode):\n",
    "        if len(test_inputs)==0:\n",
    "            print(\"Input size is 0\")\n",
    "            return\n",
    "        predicted_outputs = []\n",
    "        for t in range(len(test_inputs)):\n",
    "            outputs = self.feed_forward(test_inputs[t])\n",
    "            encoded_outputs = self.encode_output(outputs,mode)\n",
    "            predicted_outputs.append(encoded_outputs)\n",
    "        return predicted_outputs\n",
    "    \n",
    "    #this function calculates total squared error across all training sets\n",
    "    def calculate_squared_error(self,training_outputs):\n",
    "        total_error = 0.0\n",
    "        for i in range(len(training_outputs)):\n",
    "            total_error += self.output_layer.neurons[i].get_squared_error(training_outputs[i])\n",
    "        return total_error\n",
    "    \n",
    "    #this function saves all weights\n",
    "    def save_weights(self,filename):\n",
    "        fw = open(filename,\"w\")\n",
    "        fw.write(\"*****HIDDEN TO OUTPUT******\")\n",
    "        for t in range(self.num_outputs):\n",
    "            weights = self.output_layer.neurons[t].weights\n",
    "            for w in weights:\n",
    "                fw.write(str(w))\n",
    "                fw.write(\" \")\n",
    "            fw.write(\"\\n\")\n",
    "        fw.write(\"*****INPUT TO HIDDEN******\")\n",
    "        for t in range(self.num_h_layers):\n",
    "            for i in range(self.hidden_cnts[t]):\n",
    "                weights = self.hidden_layers[t].neurons[i].weights\n",
    "                for w in weights:\n",
    "                    fw.write(str(w))\n",
    "                    fw.write(\" \")\n",
    "                fw.write(\"\\n\")\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hit_rate(predicted,actual):\n",
    "    if(len(predicted)!=len(actual)):\n",
    "        print(\"Lengths do not match\")\n",
    "    hit_cnt = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]==actual[i]:\n",
    "            hit_cnt += 1\n",
    "    return float(hit_cnt)/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayScale(weights):\n",
    "    grayscale = []\n",
    "    for i in range(1,29):\n",
    "        grayscale.append(weights[28*(i-1):28*i])\n",
    "    return grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_X,test_X,train_Y,test_Y] = train_test_split()\n",
    "print(len(train_X),len(test_X),len(train_Y),len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hidden neuron counts\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "hidden_cnts = [100,150,200,250,300]\n",
    "train_rates = {}\n",
    "test_rates = {}\n",
    "num_epochs = 100\n",
    "for cnt in hidden_cnts:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"Counts:\",cnt)\n",
    "    nn = Network(0.05,0.05,0.1,1,[cnt],785,10)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],outputs[k])\n",
    "    train_preds = nn.predict(train_X,\"normal\")\n",
    "    train_rates[str(cnt)] = calc_hit_rate(train_preds,train_Y)\n",
    "    test_preds = nn.predict(test_X,\"max\")\n",
    "    test_rates[str(cnt)] = calc_hit_rate(test_preds,test_Y)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals,test_vals = [],[]\n",
    "for cnt in hidden_cnts:\n",
    "    train_vals.append(train_rates[str(cnt)])\n",
    "    test_vals.append(test_rates[str(cnt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hidden_cnts,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(hidden_cnts,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel(\"Hidden Counts\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Hidden Neuron Count vs Hit Rate(100 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//hidden_vs_hitrate2\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training learning rate\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "lrates = np.arange(0.01,0.110,0.01)\n",
    "train_rates = {}\n",
    "test_rates = {}\n",
    "num_epochs = 100\n",
    "for lrate in lrates:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"lrate:\",lrate)\n",
    "    nn = Network(lrate,lrate,0.1,1,[150],785,10)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],outputs[k])\n",
    "    train_preds = nn.predict(train_X,\"normal\")\n",
    "    train_rates[str(lrate)] = calc_hit_rate(train_preds,train_Y)\n",
    "    test_preds = nn.predict(test_X,\"max\")\n",
    "    test_rates[str(lrate)] = calc_hit_rate(test_preds,test_Y)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for lrate in lrates:\n",
    "    train_vals.append(train_rates[str(lrate)])\n",
    "    test_vals.append(test_rates[str(lrate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrates,train_vals,'-r',label=\"Training Hit Rate\")\n",
    "plt.plot(lrates,test_vals,'-b',label=\"Testing Hit Rate\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Learning Rate vs Hit Rate(100 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//learning_vs_hitrate\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training alpha value\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "alphas = [0.05,0.1,0.15,0.2,0.25]\n",
    "train_rates = {}\n",
    "test_rates = {}\n",
    "num_epochs = 100\n",
    "for alpha in alphas:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"alpha:\",alpha)\n",
    "    nn = Network(0.1,0.1,alpha,1,[150],785,10)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],outputs[k])\n",
    "    train_preds = nn.predict(train_X,\"normal\")\n",
    "    train_rates[str(alpha)] = calc_hit_rate(train_preds,train_Y)\n",
    "    test_preds = nn.predict(test_X,\"max\")\n",
    "    test_rates[str(alpha)] = calc_hit_rate(test_preds,test_Y)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for alpha in alphas:\n",
    "    train_vals.append(train_rates[str(alpha)])\n",
    "    test_vals.append(test_rates[str(alpha)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(alphas,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Alpha vs Hit Rate(100 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//alpha_vs_hitrate\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final training step using stochastic gradient descent\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "num_epochs = 1000\n",
    "hit_rates = []\n",
    "nn = Network(0.1,0.1,0.15,1,[150],785,10)#neural network with 400 random training points for each epoch\n",
    "print(\"****starting****\")\n",
    "for t in range(1,num_epochs+1):\n",
    "    [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "    training_hit_rate = 0.0\n",
    "    #epoch_start_time = timeit.default_timer()\n",
    "    for k in range(len(inputs)): #backpropagation for each point\n",
    "        nn.back_propagation(inputs[k],outputs[k])\n",
    "    if t%10==0: #calculating hit rate for every 10 epochs\n",
    "        predicted_outputs = nn.predict(inputs,\"normal\")\n",
    "        hit_rate = calc_hit_rate(predicted_outputs,outputs)\n",
    "        print(\"Epoch:%d, hit_rate:%f\" %(t,hit_rate))\n",
    "        hit_rates.append(hit_rate)\n",
    "nn.save_weights(\"final_weights_1000.txt\")\n",
    "train_preds = nn.predict(train_X,\"max\")\n",
    "train_cm = get_cm(train_Y,train_preds)\n",
    "test_preds = nn.predict(test_X,\"max\")\n",
    "test_cm = get_cm(test_Y,test_preds)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final hit-rates\n",
    "test_pred_outputs = nn.predict(test_X,\"max\")\n",
    "test_hit_rate = calc_hit_rate(test_pred_outputs,test_Y)\n",
    "\n",
    "train_pred_outputs = nn.predict(train_X,\"normal\")\n",
    "train_hit_rate = calc_hit_rate(train_pred_outputs,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving predicted labels for future use\n",
    "fw = open(\"Training_labels_1000.txt\",\"w\")\n",
    "for output in train_pred_outputs:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()\n",
    "fw = open(\"Training_labels_max_1000.txt\",\"w\")\n",
    "for output in train_pred_outputs_max:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving predicted labels for future use\n",
    "fw = open(\"Test_labels_1000.txt\",\"w\")\n",
    "for output in test_pred_outputs:\n",
    "    label = get_label(output)\n",
    "    fw.write(str(label)+\"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_outputs_max = nn.predict(train_X,\"max\")\n",
    "train_cm_max = get_cm(train_Y,train_pred_outputs_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors = 1 -np.array(hit_rates)\n",
    "plt.plot(range(1,1001,10),training_errors)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Time series of error rate(1000 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//error_timeseries(1000)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'0.08': 0.5155, '0.035': 0.46725, '0.06': 0.52425, '0.085': 0.526, '0.05': 0.51625, '0.065': 0.50625, \n",
    "#'0.005': 0.11875, '0.01': 0.23, '0.095': 0.54725, '0.03': 0.4385, '0.075': 0.53425, '0.045': 0.49175, \n",
    "#'0.04': 0.47, '0.02': 0.35725, '0.1': 0.53775, '0.055': 0.507, '0.09': 0.53975, '0.07': 0.5155, '0.015': 0.3165, '0.025': 0.4135}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'100': 0.4545, '200': 0.482, '150': 0.48975}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validating learning rates\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "lrates = np.arange(0.01,0.210,0.02)\n",
    "train_errors = {}\n",
    "test_errors = {}\n",
    "num_epochs = 50\n",
    "for lrate in lrates:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"lrate:\",lrate)\n",
    "    nn = Network(lrate,lrate,0.1,1,[150],785,785)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],inputs[k])\n",
    "    train_preds = nn.predict(train_X,\"none\")\n",
    "    train_errors[str(lrate)] = get_error(train_preds,train_X)\n",
    "    test_preds = nn.predict(test_X,\"none\")\n",
    "    test_errors[str(lrate)] = get_error(test_preds,test_X)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for lrate in lrates:\n",
    "    train_vals.append(train_errors[str(lrate)])\n",
    "    test_vals.append(test_errors[str(lrate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrates = np.arange(0.01,0.210,0.02)\n",
    "train_vals = [99208.30,57307.14,43604.76,37749.79,34093.12,31666.06,31004.18,30736.009,32775.18,33264.38]\n",
    "test_vals = [25231.35,14910.60,11579.97,10199.35,9281.81,8720.66,8627.07,8548.89,9058.94,9310.46]\n",
    "train_vals = [vals/4000 for vals in train_vals]\n",
    "test_vals = [vals/1000 for vals in test_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrates,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(lrates,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Normalized Squared Error\")\n",
    "plt.title(\"Learning Rate vs Squared Error(50 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//lrate_vs_squarederror\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100,301,50):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validating alpha\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[100],num_inputs=1,num_outputs=1\n",
    "alphas = [0.05,0.1,0.15,0.2,0.25]\n",
    "train_errors = {}\n",
    "test_errors = {}\n",
    "num_epochs = 50\n",
    "for alpha in alphas:\n",
    "    train_hit_rate = 0.0\n",
    "    test_hit_rate = 0.0\n",
    "    print(\"alpha:\",alpha)\n",
    "    nn = Network(0.15,0.15,alpha,1,[150],785,785)#neural network with 400 random training points for each epoch\n",
    "    for t in range(1,num_epochs+1):\n",
    "        if(t%10==0):\n",
    "            print(\"Epoch:\",t)\n",
    "        [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "        #epoch_start_time = timeit.default_timer()\n",
    "        for k in range(len(inputs)): #backpropagation for each point\n",
    "            nn.back_propagation(inputs[k],inputs[k])\n",
    "    train_preds = nn.predict(train_X,\"none\")\n",
    "    train_errors[str(alpha)] = get_error(train_preds,train_X)\n",
    "    test_preds = nn.predict(test_X,\"none\")\n",
    "    test_errors[str(alpha)] = get_error(test_preds,test_X)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals, test_vals = [],[]\n",
    "for alpha in alphas:\n",
    "    train_vals.append(train_errors[str(alpha)])\n",
    "    test_vals.append(test_errors[str(alpha)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.05,0.1,0.15,0.2,0.25]\n",
    "train_vals = [31101.07,31750.68,31264.08,30610.89,31794.04]\n",
    "test_vals = [8682.46,8806.08,8668.63,8638.38,8980.46]\n",
    "train_vals = [vals/4000 for vals in train_vals]\n",
    "test_vals = [vals/1000 for vals in test_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,train_vals,'-r',label=\"Training\")\n",
    "plt.plot(alphas,test_vals,'-b',label=\"Testing\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Normalized Squared Error\")\n",
    "plt.title(\"Alpha vs Squared Error(50 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//alpha_vs_squarederror\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final training step using stochastic gradient descent\n",
    "#self,lrate_h,lrate_o,alpha,num_h_layers=1,hidden_cnts=[150],num_inputs=1,num_outputs=1\n",
    "num_epochs = 500\n",
    "errors = []\n",
    "nn = Network(0.15,0.15,0.20,1,[150],785,785)#neural network with 400 random training points for each epoch\n",
    "print(\"****starting****\")\n",
    "for t in range(1,num_epochs+1):\n",
    "    [inputs,outputs] = random_split(train_X,train_Y,0.025)\n",
    "    training_hit_rate = 0.0\n",
    "    #epoch_start_time = timeit.default_timer()\n",
    "    for k in range(len(inputs)): #backpropagation for each point\n",
    "        nn.back_propagation(inputs[k],inputs[k])\n",
    "    if t%10==0: #calculating hit rate for every 10 epochs\n",
    "        predicted_outputs = nn.predict(inputs,\"none\")\n",
    "        error = get_error(predicted_outputs,inputs)\n",
    "        print(\"Epoch:%d, error:%f\" %(t,error))\n",
    "        errors.append(error)\n",
    "nn.save_weights(\"auto_encoder_final_weights_500.txt\")\n",
    "train_preds = nn.predict(train_X,\"none\")\n",
    "train_error = get_error(train_preds,train_X)\n",
    "test_preds = nn.predict(test_X,\"none\")\n",
    "test_error = get_error(test_preds,test_X)\n",
    "print(\"Final Training Error:%f Testing Error:%f\" %(train_error,test_error))\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_weights = nn.get_hidden_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Training Error:%f Testing Error:%f\" %(train_error,test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_errors = [(error/100)/784 for error in errors] #normalizing errors by number of points\n",
    "plt.plot(range(1,1001,10),norm_errors)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Normalized Squared Error per neuron\")\n",
    "plt.title(\"Time series of squared error(1000 Epochs)\")\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//squarederror_timeseries(1000)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and testing errors\n",
    "train_error = (train_error)/4000 #normalizing the squared error\n",
    "test_error = (test_error)/1000\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "y_pos =[0.5,1]\n",
    "bars = [train_error,test_error]\n",
    "plt.bar(y_pos, bars, align='center', alpha=0.4,width=0.1)\n",
    "plt.xticks(y_pos,[\"Training Error\",\"Testing Error\"])\n",
    "plt.ylabel('Normalized Squared Error')\n",
    "plt.title('Training & Testing Squared Errors')\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//squared_error_bars(1000).png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_errors = []\n",
    "labeled_test_errors = []\n",
    "for i in range(10):\n",
    "    tr_error = get_labeled_error(train_preds,train_X,train_Y,i)\n",
    "    labeled_train_errors.append(tr_error)\n",
    "    te_error = get_labeled_error(test_preds,test_X,test_Y,i)\n",
    "    labeled_test_errors.append(te_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(1,11)\n",
    "labels = np.arange(10)\n",
    "plt.bar(y_pos,labeled_train_errors,align='center',alpha=0.4,width=0.3,color='r')\n",
    "plt.bar(y_pos+0.3,labeled_test_errors,align='center',alpha=0.4,width=0.3,color='y')\n",
    "plt.legend(('Training','Testing'))\n",
    "plt.xticks(y_pos,labels)\n",
    "plt.xlabel('Digits')\n",
    "plt.ylabel('Normalized Squared Error')\n",
    "plt.title('Digit-wise Squared Error')\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//digitwise_bars(1000).png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for i in range(150):\n",
    "    all_weights.append(nn.hidden_layers[0].neurons[i].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = all_weights[0]\n",
    "sample_weights = sample_weights[1:785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_scale_weights = []\n",
    "for i in range(10):\n",
    "    gray_scale_weights.append(convert_to_grayScale(all_weights[i]))\n",
    "print(len(gray_scale_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,12)\n",
    "fig = plt.figure()\n",
    "grids = gridspec.GridSpec(10, 10, wspace=0.0)\n",
    "ax = [plt.subplot(grids[i]) for i in range(100)]\n",
    "grids.update(hspace=0)\n",
    "for i in range(100):\n",
    "    ax[i].imshow(convert_to_grayScale(all_weights[i]),cmap = plt.get_cmap('gray'))\n",
    "    ax[i].axis('off')\n",
    "plt.savefig(\"C://Users//kisha//workspace//Assignment_3_Intelligent_Systems//feature_grid(1000).png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
